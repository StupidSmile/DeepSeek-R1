{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_pytorch_workflow_exercises.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StupidSmile/DeepSeek-R1/blob/main/extras/exercises/01_pytorch_workflow_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 01. PyTorch Workflow Exercise Template\n",
        "\n",
        "The following is a template for the PyTorch workflow exercises.\n",
        "\n",
        "It's only starter code and it's your job to fill in the blanks.\n",
        "\n",
        "Because of the flexibility of PyTorch, there may be more than one way to answer the question.\n",
        "\n",
        "Don't worry about trying to be *right* just try writing code that suffices the question.\n",
        "\n",
        "You can see one form of [solutions on GitHub](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/extras/solutions) (but try the exercises below yourself first!)."
      ],
      "metadata": {
        "id": "N8LsPXZti9Sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.__version__"
      ],
      "metadata": {
        "id": "Glu2fM4dkNlx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "6b455769-85b4-4cb0-a3c2-898cae1d9457"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.6.0+cu124'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device-agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "LqKhXY26m31s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14f5755e-0952-4709-c344-a341ccea4053"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Create a straight line dataset using the linear regression formula (`weight * X + bias`).\n",
        "  * Set `weight=0.3` and `bias=0.9` there should be at least 100 datapoints total.\n",
        "  * Split the data into 80% training, 20% testing.\n",
        "  * Plot the training and testing data so it becomes visual.\n",
        "\n",
        "Your output of the below cell should look something like:\n",
        "```\n",
        "Number of X samples: 100\n",
        "Number of y samples: 100\n",
        "First 10 X & y samples:\n",
        "X: tensor([0.0000, 0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800,\n",
        "        0.0900])\n",
        "y: tensor([0.9000, 0.9030, 0.9060, 0.9090, 0.9120, 0.9150, 0.9180, 0.9210, 0.9240,\n",
        "        0.9270])\n",
        "```\n",
        "\n",
        "Of course the numbers in `X` and `y` may be different but ideally they're created using the linear regression formula."
      ],
      "metadata": {
        "id": "g7HUhxCxjeBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data parameters\n",
        "weight=0.3\n",
        "bias=0.9\n",
        "# Make X and y using linear regression feature\n",
        "start=0\n",
        "end=1\n",
        "stop=0.01\n",
        "\n",
        "X = torch.arange(start, end, stop)\n",
        "y = weight * X + bias\n",
        "\n",
        "print(f\"Number of X samples: {len(X)}\")\n",
        "print(f\"Number of y samples: {len(y)}\")\n",
        "print(f\"First 10 X & y samples:\\nX: {X[:10]}\\ny: {y[:10]}\")"
      ],
      "metadata": {
        "id": "KbDG5MV7jhvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0f459e7-1403-4265-9d46-e453ec10652b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of X samples: 100\n",
            "Number of y samples: 100\n",
            "First 10 X & y samples:\n",
            "X: tensor([0.0000, 0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800,\n",
            "        0.0900])\n",
            "y: tensor([0.9000, 0.9030, 0.9060, 0.9090, 0.9120, 0.9150, 0.9180, 0.9210, 0.9240,\n",
            "        0.9270])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing\n",
        "train_split = int(0.8 * len(X))\n",
        "X_train, y_train = X[:train_split], y[:train_split]\n",
        "X_test, y_test = X[train_split:], y[train_split:]\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ],
      "metadata": {
        "id": "GlwtT1djkmLw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0482212b-37d7-4c2d-b2e8-a26162f1dd09"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80, 80, 20, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "source": [
        "# Plot the training and testing data\n",
        "def plot_predictions(X_train,X_test, y_train, y_test, predictions=None): # Added predictions=None as a default parameter\n",
        "  plt.figure(figsize=(10,7))\n",
        "\n",
        "  plt.scatter(X_train, y_train , c=\"b\", s=4,label= \"Training data\")\n",
        "\n",
        "  plt.scatter(X_test, y_test ,c=\"g\", s=4, label=\"Testing data\")\n",
        "\n",
        "  if predictions is not None:\n",
        "    plt.scatter(X_test, predictions,c=\"r\", s=4, label=\"Predictions\")\n",
        "\n",
        "plot_predictions(X_train,X_test, y_train, y_test);"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "eUjWlB4xNRNo",
        "outputId": "d351a3ba-fb79-456b-a7e4-6b2482ca9e2d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Build a PyTorch model by subclassing `nn.Module`.\n",
        "  * Inside should be a randomly initialized `nn.Parameter()` with `requires_grad=True`, one for `weights` and one for `bias`.\n",
        "  * Implement the `forward()` method to compute the linear regression function you used to create the dataset in 1.\n",
        "  * Once you've constructed the model, make an instance of it and check its `state_dict()`.\n",
        "  * **Note:** If you'd like to use `nn.Linear()` instead of `nn.Parameter()` you can."
      ],
      "metadata": {
        "id": "ImZoe3v8jif8"
      }
    },
    {
      "source": [
        "# Create PyTorch linear regression model by subclassing nn.Module\n",
        "import torch\n",
        "import torch.nn as nn # Import nn explicitly\n",
        "\n",
        "class LinearRegressionModel(nn.Module): # Inherit from nn.Module\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "      self.weights = nn.Parameter(torch.randn(1,\n",
        "                                              dtype=torch.float,\n",
        "                                              requires_grad=True))\n",
        "      self.bias = nn.Parameter(torch.randn(1,\n",
        "                                              dtype=torch.float,\n",
        "                                              requires_grad=True))\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor: # <- \"x\" is the input data (e.g. training/testing features)\n",
        "        return self.weights * x + self.bias # <- this is the linear regression formula (y = m*x + b)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "cOmPxpfhO4s8"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model and put it to the target device\n",
        "RANDOM_SEED =42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "model= LinearRegressionModel()\n",
        "list(model.parameters())"
      ],
      "metadata": {
        "id": "5LdcDnmOmyQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7122e314-4858-42c3-9d79-33328684fcd2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([0.3367], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([0.1288], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0wu6EXzO-xN",
        "outputId": "188de756-36cd-44f4-877d-cdb419c85a84"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Create a loss function and optimizer using `nn.L1Loss()` and `torch.optim.SGD(params, lr)` respectively.\n",
        "  * Set the learning rate of the optimizer to be 0.01 and the parameters to optimize should be the model parameters from the model you created in 2.\n",
        "  * Write a training loop to perform the appropriate training steps for 300 epochs.\n",
        "  * The training loop should test the model on the test dataset every 20 epochs."
      ],
      "metadata": {
        "id": "G6nYOrJhjtfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the loss function\n",
        "loss_fn = nn.L1Loss() # MAE loss is same as L1Loss\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = torch.optim.SGD(params=model.parameters(), # parameters of target model to optimize\n",
        "                            lr=0.01) # learning rate (how much the optimizer should change parameters at each step, higher=more (less stable), lower=less (might take a long time))"
      ],
      "metadata": {
        "id": "y9ZriT6jPIHY"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Train model for 300 epochs\n",
        "epochs=1000\n",
        "\n",
        "# Send data to target device\n",
        "# Create empty loss lists to track values\n",
        "train_loss_values = []\n",
        "test_loss_values = []\n",
        "epoch_count = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  ### Training\n",
        "\n",
        "  # Put model in train mode\n",
        "  model.train()\n",
        "\n",
        "  # 1. Forward pass\n",
        "  y_pred = model(X_train)\n",
        "\n",
        "  # 2. Calculate loss\n",
        "  loss = loss_fn(y_pred, y_train)\n",
        "\n",
        "  # 3. Zero gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. Backpropagation\n",
        "  loss.backward()\n",
        "\n",
        "  # 5. Step the optimizer\n",
        "  optimizer.step()\n",
        "\n",
        "  ### Perform testing every 20 epochs\n",
        "  if epoch % 20 == 0:\n",
        "\n",
        "  # Put model in evaluation mode and setup inference context\n",
        "   model.eval()\n",
        "    # 1. Forward pass\n",
        "   test_pred = model(X_test)\n",
        "      # 2. Calculate test loss\n",
        "   test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type\n",
        "\n",
        "      # Print out what's happening\n",
        "  print(f\"Epoch: {epoch} | Train loss: {loss:.3f} | Test loss: {test_loss:.3f}\")"
      ],
      "metadata": {
        "id": "xpE83NvNnkdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9df49471-5838-4700-b1ce-90413643347e"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 1 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 2 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 3 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 4 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 5 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 6 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 7 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 8 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 9 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 10 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 11 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 12 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 13 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 14 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 15 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 16 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 17 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 18 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 19 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 20 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 21 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 22 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 23 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 24 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 25 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 26 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 27 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 28 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 29 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 30 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 31 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 32 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 33 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 34 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 35 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 36 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 37 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 38 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 39 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 40 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 41 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 42 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 43 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 44 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 45 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 46 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 47 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 48 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 49 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 50 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 51 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 52 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 53 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 54 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 55 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 56 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 57 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 58 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 59 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 60 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 61 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 62 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 63 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 64 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 65 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 66 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 67 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 68 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 69 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 70 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 71 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 72 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 73 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 74 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 75 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 76 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 77 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 78 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 79 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 80 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 81 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 82 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 83 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 84 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 85 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 86 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 87 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 88 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 89 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 90 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 91 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 92 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 93 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 94 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 95 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 96 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 97 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 98 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 99 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 100 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 101 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 102 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 103 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 104 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 105 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 106 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 107 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 108 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 109 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 110 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 111 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 112 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 113 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 114 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 115 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 116 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 117 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 118 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 119 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 120 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 121 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 122 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 123 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 124 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 125 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 126 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 127 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 128 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 129 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 130 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 131 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 132 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 133 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 134 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 135 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 136 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 137 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 138 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 139 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 140 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 141 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 142 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 143 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 144 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 145 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 146 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 147 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 148 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 149 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 150 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 151 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 152 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 153 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 154 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 155 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 156 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 157 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 158 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 159 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 160 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 161 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 162 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 163 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 164 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 165 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 166 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 167 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 168 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 169 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 170 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 171 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 172 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 173 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 174 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 175 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 176 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 177 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 178 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 179 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 180 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 181 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 182 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 183 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 184 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 185 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 186 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 187 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 188 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 189 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 190 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 191 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 192 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 193 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 194 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 195 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 196 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 197 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 198 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 199 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 200 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 201 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 202 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 203 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 204 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 205 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 206 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 207 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 208 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 209 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 210 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 211 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 212 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 213 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 214 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 215 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 216 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 217 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 218 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 219 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 220 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 221 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 222 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 223 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 224 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 225 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 226 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 227 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 228 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 229 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 230 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 231 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 232 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 233 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 234 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 235 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 236 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 237 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 238 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 239 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 240 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 241 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 242 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 243 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 244 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 245 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 246 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 247 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 248 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 249 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 250 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 251 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 252 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 253 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 254 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 255 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 256 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 257 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 258 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 259 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 260 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 261 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 262 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 263 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 264 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 265 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 266 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 267 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 268 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 269 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 270 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 271 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 272 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 273 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 274 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 275 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 276 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 277 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 278 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 279 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 280 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 281 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 282 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 283 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 284 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 285 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 286 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 287 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 288 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 289 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 290 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 291 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 292 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 293 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 294 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 295 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 296 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 297 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 298 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 299 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 300 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 301 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 302 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 303 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 304 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 305 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 306 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 307 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 308 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 309 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 310 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 311 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 312 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 313 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 314 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 315 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 316 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 317 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 318 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 319 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 320 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 321 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 322 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 323 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 324 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 325 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 326 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 327 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 328 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 329 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 330 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 331 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 332 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 333 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 334 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 335 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 336 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 337 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 338 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 339 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 340 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 341 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 342 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 343 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 344 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 345 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 346 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 347 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 348 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 349 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 350 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 351 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 352 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 353 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 354 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 355 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 356 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 357 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 358 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 359 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 360 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 361 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 362 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 363 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 364 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 365 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 366 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 367 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 368 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 369 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 370 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 371 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 372 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 373 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 374 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 375 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 376 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 377 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 378 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 379 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 380 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 381 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 382 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 383 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 384 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 385 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 386 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 387 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 388 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 389 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 390 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 391 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 392 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 393 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 394 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 395 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 396 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 397 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 398 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 399 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 400 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 401 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 402 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 403 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 404 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 405 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 406 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 407 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 408 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 409 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 410 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 411 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 412 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 413 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 414 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 415 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 416 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 417 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 418 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 419 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 420 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 421 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 422 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 423 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 424 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 425 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 426 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 427 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 428 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 429 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 430 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 431 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 432 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 433 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 434 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 435 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 436 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 437 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 438 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 439 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 440 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 441 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 442 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 443 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 444 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 445 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 446 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 447 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 448 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 449 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 450 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 451 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 452 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 453 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 454 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 455 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 456 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 457 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 458 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 459 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 460 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 461 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 462 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 463 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 464 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 465 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 466 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 467 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 468 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 469 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 470 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 471 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 472 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 473 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 474 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 475 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 476 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 477 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 478 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 479 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 480 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 481 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 482 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 483 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 484 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 485 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 486 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 487 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 488 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 489 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 490 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 491 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 492 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 493 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 494 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 495 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 496 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 497 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 498 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 499 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 500 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 501 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 502 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 503 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 504 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 505 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 506 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 507 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 508 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 509 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 510 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 511 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 512 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 513 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 514 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 515 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 516 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 517 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 518 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 519 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 520 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 521 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 522 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 523 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 524 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 525 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 526 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 527 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 528 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 529 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 530 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 531 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 532 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 533 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 534 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 535 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 536 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 537 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 538 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 539 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 540 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 541 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 542 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 543 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 544 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 545 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 546 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 547 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 548 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 549 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 550 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 551 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 552 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 553 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 554 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 555 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 556 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 557 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 558 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 559 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 560 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 561 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 562 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 563 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 564 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 565 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 566 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 567 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 568 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 569 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 570 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 571 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 572 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 573 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 574 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 575 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 576 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 577 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 578 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 579 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 580 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 581 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 582 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 583 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 584 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 585 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 586 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 587 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 588 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 589 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 590 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 591 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 592 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 593 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 594 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 595 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 596 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 597 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 598 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 599 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 600 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 601 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 602 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 603 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 604 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 605 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 606 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 607 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 608 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 609 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 610 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 611 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 612 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 613 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 614 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 615 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 616 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 617 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 618 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 619 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 620 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 621 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 622 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 623 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 624 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 625 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 626 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 627 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 628 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 629 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 630 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 631 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 632 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 633 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 634 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 635 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 636 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 637 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 638 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 639 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 640 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 641 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 642 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 643 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 644 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 645 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 646 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 647 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 648 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 649 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 650 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 651 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 652 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 653 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 654 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 655 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 656 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 657 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 658 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 659 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 660 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 661 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 662 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 663 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 664 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 665 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 666 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 667 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 668 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 669 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 670 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 671 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 672 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 673 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 674 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 675 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 676 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 677 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 678 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 679 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 680 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 681 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 682 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 683 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 684 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 685 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 686 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 687 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 688 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 689 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 690 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 691 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 692 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 693 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 694 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 695 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 696 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 697 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 698 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 699 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 700 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 701 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 702 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 703 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 704 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 705 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 706 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 707 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 708 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 709 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 710 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 711 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 712 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 713 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 714 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 715 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 716 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 717 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 718 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 719 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 720 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 721 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 722 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 723 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 724 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 725 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 726 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 727 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 728 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 729 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 730 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 731 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 732 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 733 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 734 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 735 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 736 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 737 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 738 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 739 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 740 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 741 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 742 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 743 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 744 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 745 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 746 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 747 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 748 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 749 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 750 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 751 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 752 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 753 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 754 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 755 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 756 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 757 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 758 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 759 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 760 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 761 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 762 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 763 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 764 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 765 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 766 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 767 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 768 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 769 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 770 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 771 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 772 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 773 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 774 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 775 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 776 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 777 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 778 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 779 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 780 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 781 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 782 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 783 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 784 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 785 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 786 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 787 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 788 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 789 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 790 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 791 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 792 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 793 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 794 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 795 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 796 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 797 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 798 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 799 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 800 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 801 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 802 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 803 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 804 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 805 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 806 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 807 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 808 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 809 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 810 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 811 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 812 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 813 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 814 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 815 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 816 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 817 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 818 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 819 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 820 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 821 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 822 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 823 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 824 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 825 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 826 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 827 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 828 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 829 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 830 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 831 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 832 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 833 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 834 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 835 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 836 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 837 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 838 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 839 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 840 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 841 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 842 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 843 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 844 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 845 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 846 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 847 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 848 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 849 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 850 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 851 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 852 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 853 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 854 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 855 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 856 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 857 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 858 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 859 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 860 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 861 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 862 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 863 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 864 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 865 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 866 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 867 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 868 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 869 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 870 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 871 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 872 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 873 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 874 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 875 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 876 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 877 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 878 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 879 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 880 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 881 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 882 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 883 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 884 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 885 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 886 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 887 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 888 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 889 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 890 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 891 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 892 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 893 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 894 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 895 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 896 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 897 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 898 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 899 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 900 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 901 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 902 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 903 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 904 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 905 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 906 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 907 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 908 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 909 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 910 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 911 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 912 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 913 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 914 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 915 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 916 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 917 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 918 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 919 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 920 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 921 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 922 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 923 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 924 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 925 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 926 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 927 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 928 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 929 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 930 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 931 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 932 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 933 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 934 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 935 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 936 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 937 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 938 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 939 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 940 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 941 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 942 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 943 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 944 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 945 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 946 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 947 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 948 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 949 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 950 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 951 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 952 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 953 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 954 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 955 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 956 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 957 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 958 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 959 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 960 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 961 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 962 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 963 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 964 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 965 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 966 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 967 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 968 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 969 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 970 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 971 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 972 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 973 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 974 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 975 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 976 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 977 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 978 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 979 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 980 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 981 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 982 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 983 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 984 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 985 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 986 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 987 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 988 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 989 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 990 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 991 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 992 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 993 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 994 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 995 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 996 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 997 | Train loss: 0.008 | Test loss: 0.006\n",
            "Epoch: 998 | Train loss: 0.004 | Test loss: 0.006\n",
            "Epoch: 999 | Train loss: 0.008 | Test loss: 0.006\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# Training loop\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Train model for 300 epochs\n",
        "epochs=1000\n",
        "\n",
        "# Send data to target device\n",
        "# Create empty loss lists to track values\n",
        "train_loss_values = []\n",
        "test_loss_values = []\n",
        "epoch_count = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  ### Training\n",
        "\n",
        "  # Put model in train mode\n",
        "  model.train()\n",
        "\n",
        "  # 1. Forward pass\n",
        "  y_pred = model(X_train)\n",
        "\n",
        "  # 2. Calculate loss\n",
        "  loss = loss_fn(y_pred, y_train)\n",
        "\n",
        "  # 3. Zero gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. Backpropagation\n",
        "  loss.backward()\n",
        "\n",
        "  # 5. Step the optimizer\n",
        "  optimizer.step()\n",
        "\n",
        "  # for plotting the loss\n",
        "  train_loss_values.append(loss.item())\n",
        "\n",
        "  ### Perform testing every 20 epochs\n",
        "  if epoch % 20 == 0:\n",
        "\n",
        "    # Put model in evaluation mode and setup inference context\n",
        "    model.eval()\n",
        "    # 1. Forward pass\n",
        "    test_pred = model(X_test)\n",
        "    # 2. Calculate test loss\n",
        "    test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type\n",
        "\n",
        "    # for plotting the loss\n",
        "    test_loss_values.append(test_loss.item())\n",
        "    epoch_count.append(epoch) # for plotting the loss against epoch number\n",
        "\n",
        "    #"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "iza19FgoQuM4"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtCknbHzQQJr",
        "outputId": "3f3dcc1b-9164-45c0-f72d-08f26f1804f8"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.3067])), ('bias', tensor([0.9011]))])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss curves\n",
        "plt.plot(epochs, train_loss_values, label=\"Train loss\")\n",
        "plt.plot(epochs, test_loss_values, label=\"Test loss\")\n",
        "plt.title(\"Training and test loss curves\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830
        },
        "id": "oVzKuZJTQZ2h",
        "outputId": "acac2fea-836b-4234-e1f8-bf015ccda91f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "x and y must have same first dimension, but have shapes (1,) and (1000,)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-c0f7dfd8056d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot the loss curves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Train loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Test loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training and test loss curves\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3827\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3828\u001b[0m ) -> list[Line2D]:\n\u001b[0;32m-> 3829\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   3830\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3831\u001b[0m         \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \"\"\"\n\u001b[1;32m   1776\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1777\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1778\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             yield from self._plot_args(\n\u001b[0m\u001b[1;32m    298\u001b[0m                 \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mambiguous_fmt_datakey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mambiguous_fmt_datakey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0mreturn_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    495\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1,) and (1000,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGiCAYAAADA0E3hAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHMNJREFUeJzt3W9s3VX9wPFP29FbCLRM59ptFisoogIbbqwWJIipNoFM98A4wWxz4Y/gJLhGZWOwiug6EciiKy5MEB+omxAwxi1DrC4GqVnY1gRkg8DATWMLE9fOIi1rv78Hhvqr62C39M9O+3ol98GO59zvuR5G39x/LciyLAsAgAQUjvUGAACOlXABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkpF3uPzhD3+IefPmxfTp06OgoCB++ctfvuWabdu2xUc+8pHI5XLxvve9L+6///4hbBUAmOjyDpeurq6YOXNmNDU1HdP8F154IS677LK45JJLorW1Nb761a/GVVddFY888kjemwUAJraCt/NLFgsKCuLhhx+O+fPnH3XOjTfeGJs3b46nnnqqf+zzn/98HDx4MLZu3TrUSwMAE9Ckkb5AS0tL1NbWDhirq6uLr371q0dd093dHd3d3f1/7uvri1deeSXe+c53RkFBwUhtFQAYRlmWxaFDh2L69OlRWDg8b6sd8XBpa2uL8vLyAWPl5eXR2dkZ//73v+PEE088Yk1jY2PceuutI701AGAU7N+/P9797ncPy32NeLgMxYoVK6K+vr7/zx0dHXHaaafF/v37o7S0dAx3BgAcq87OzqisrIxTTjll2O5zxMOloqIi2tvbB4y1t7dHaWnpoM+2RETkcrnI5XJHjJeWlgoXAEjMcL7NY8S/x6Wmpiaam5sHjD366KNRU1Mz0pcGAMaZvMPlX//6V7S2tkZra2tE/Ofjzq2trbFv376I+M/LPIsWLeqff+2118bevXvjG9/4RuzZsyfuvvvu+MUvfhHLli0bnkcAAEwYeYfLE088Eeedd16cd955ERFRX18f5513XqxatSoiIv7+97/3R0xExHvf+97YvHlzPProozFz5sy4884740c/+lHU1dUN00MAACaKt/U9LqOls7MzysrKoqOjw3tcACARI/Hz2+8qAgCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGUMKl6ampqiqqoqSkpKorq6O7du3v+n8tWvXxgc+8IE48cQTo7KyMpYtWxavvfbakDYMAExceYfLpk2bor6+PhoaGmLnzp0xc+bMqKuri5deemnQ+T/72c9i+fLl0dDQELt374577703Nm3aFDfddNPb3jwAMLHkHS533XVXXH311bFkyZL40Ic+FOvXr4+TTjop7rvvvkHnP/7443HhhRfGFVdcEVVVVfGpT30qLr/88rd8lgYA4H/lFS49PT2xY8eOqK2t/e8dFBZGbW1ttLS0DLrmggsuiB07dvSHyt69e2PLli1x6aWXHvU63d3d0dnZOeAGADApn8kHDhyI3t7eKC8vHzBeXl4ee/bsGXTNFVdcEQcOHIiPfexjkWVZHD58OK699to3famosbExbr311ny2BgBMACP+qaJt27bF6tWr4+67746dO3fGQw89FJs3b47bbrvtqGtWrFgRHR0d/bf9+/eP9DYBgATk9YzLlClToqioKNrb2weMt7e3R0VFxaBrbrnllli4cGFcddVVERFxzjnnRFdXV1xzzTWxcuXKKCw8sp1yuVzkcrl8tgYATAB5PeNSXFwcs2fPjubm5v6xvr6+aG5ujpqamkHXvPrqq0fESVFRUUREZFmW734BgAksr2dcIiLq6+tj8eLFMWfOnJg7d26sXbs2urq6YsmSJRERsWjRopgxY0Y0NjZGRMS8efPirrvuivPOOy+qq6vjueeei1tuuSXmzZvXHzAAAMci73BZsGBBvPzyy7Fq1apoa2uLWbNmxdatW/vfsLtv374Bz7DcfPPNUVBQEDfffHP87W9/i3e9610xb968+M53vjN8jwIAmBAKsgRer+ns7IyysrLo6OiI0tLSsd4OAHAMRuLnt99VBAAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoYULk1NTVFVVRUlJSVRXV0d27dvf9P5Bw8ejKVLl8a0adMil8vFmWeeGVu2bBnShgGAiWtSvgs2bdoU9fX1sX79+qiuro61a9dGXV1dPPPMMzF16tQj5vf09MQnP/nJmDp1ajz44IMxY8aM+Mtf/hKnnnrqcOwfAJhACrIsy/JZUF1dHeeff36sW7cuIiL6+vqisrIyrr/++li+fPkR89evXx/f+973Ys+ePXHCCScMaZOdnZ1RVlYWHR0dUVpaOqT7AABG10j8/M7rpaKenp7YsWNH1NbW/vcOCgujtrY2WlpaBl3zq1/9KmpqamLp0qVRXl4eZ599dqxevTp6e3uPep3u7u7o7OwccAMAyCtcDhw4EL29vVFeXj5gvLy8PNra2gZds3fv3njwwQejt7c3tmzZErfcckvceeed8e1vf/uo12lsbIyysrL+W2VlZT7bBADGqRH/VFFfX19MnTo17rnnnpg9e3YsWLAgVq5cGevXrz/qmhUrVkRHR0f/bf/+/SO9TQAgAXm9OXfKlClRVFQU7e3tA8bb29ujoqJi0DXTpk2LE044IYqKivrHPvjBD0ZbW1v09PREcXHxEWtyuVzkcrl8tgYATAB5PeNSXFwcs2fPjubm5v6xvr6+aG5ujpqamkHXXHjhhfHcc89FX19f/9izzz4b06ZNGzRaAACOJu+Xiurr62PDhg3xk5/8JHbv3h3XXXdddHV1xZIlSyIiYtGiRbFixYr++dddd1288sorccMNN8Szzz4bmzdvjtWrV8fSpUuH71EAABNC3t/jsmDBgnj55Zdj1apV0dbWFrNmzYqtW7f2v2F33759UVj43x6qrKyMRx55JJYtWxbnnntuzJgxI2644Ya48cYbh+9RAAATQt7f4zIWfI8LAKRnzL/HBQBgLAkXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASMaQwqWpqSmqqqqipKQkqqurY/v27ce0buPGjVFQUBDz588fymUBgAku73DZtGlT1NfXR0NDQ+zcuTNmzpwZdXV18dJLL73puhdffDG+9rWvxUUXXTTkzQIAE1ve4XLXXXfF1VdfHUuWLIkPfehDsX79+jjppJPivvvuO+qa3t7e+MIXvhC33nprnH766W95je7u7ujs7BxwAwDIK1x6enpix44dUVtb+987KCyM2traaGlpOeq6b33rWzF16tS48sorj+k6jY2NUVZW1n+rrKzMZ5sAwDiVV7gcOHAgent7o7y8fMB4eXl5tLW1Dbrmsccei3vvvTc2bNhwzNdZsWJFdHR09N/279+fzzYBgHFq0kje+aFDh2LhwoWxYcOGmDJlyjGvy+VykcvlRnBnAECK8gqXKVOmRFFRUbS3tw8Yb29vj4qKiiPmP//88/Hiiy/GvHnz+sf6+vr+c+FJk+KZZ56JM844Yyj7BgAmoLxeKiouLo7Zs2dHc3Nz/1hfX180NzdHTU3NEfPPOuusePLJJ6O1tbX/9ulPfzouueSSaG1t9d4VACAveb9UVF9fH4sXL445c+bE3LlzY+3atdHV1RVLliyJiIhFixbFjBkzorGxMUpKSuLss88esP7UU0+NiDhiHADgreQdLgsWLIiXX345Vq1aFW1tbTFr1qzYunVr/xt29+3bF4WFvpAXABh+BVmWZWO9ibfS2dkZZWVl0dHREaWlpWO9HQDgGIzEz29PjQAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkIwhhUtTU1NUVVVFSUlJVFdXx/bt2486d8OGDXHRRRfF5MmTY/LkyVFbW/um8wEAjibvcNm0aVPU19dHQ0ND7Ny5M2bOnBl1dXXx0ksvDTp/27Ztcfnll8fvf//7aGlpicrKyvjUpz4Vf/vb39725gGAiaUgy7IsnwXV1dVx/vnnx7p16yIioq+vLyorK+P666+P5cuXv+X63t7emDx5cqxbty4WLVo06Jzu7u7o7u7u/3NnZ2dUVlZGR0dHlJaW5rNdAGCMdHZ2RllZ2bD+/M7rGZeenp7YsWNH1NbW/vcOCgujtrY2Wlpajuk+Xn311Xj99dfjHe94x1HnNDY2RllZWf+tsrIyn20CAONUXuFy4MCB6O3tjfLy8gHj5eXl0dbWdkz3ceONN8b06dMHxM//WrFiRXR0dPTf9u/fn882AYBxatJoXmzNmjWxcePG2LZtW5SUlBx1Xi6Xi1wuN4o7AwBSkFe4TJkyJYqKiqK9vX3AeHt7e1RUVLzp2jvuuCPWrFkTv/3tb+Pcc8/Nf6cAwISX10tFxcXFMXv27Ghubu4f6+vri+bm5qipqTnquttvvz1uu+222Lp1a8yZM2fouwUAJrS8Xyqqr6+PxYsXx5w5c2Lu3Lmxdu3a6OrqiiVLlkRExKJFi2LGjBnR2NgYERHf/e53Y9WqVfGzn/0sqqqq+t8Lc/LJJ8fJJ588jA8FABjv8g6XBQsWxMsvvxyrVq2Ktra2mDVrVmzdurX/Dbv79u2LwsL/PpHzwx/+MHp6euKzn/3sgPtpaGiIb37zm29v9wDAhJL397iMhZH4HDgAMLLG/HtcAADGknABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAwpXJqamqKqqipKSkqiuro6tm/f/qbzH3jggTjrrLOipKQkzjnnnNiyZcuQNgsATGx5h8umTZuivr4+GhoaYufOnTFz5syoq6uLl156adD5jz/+eFx++eVx5ZVXxq5du2L+/Pkxf/78eOqpp9725gGAiaUgy7IsnwXV1dVx/vnnx7p16yIioq+vLyorK+P666+P5cuXHzF/wYIF0dXVFb/+9a/7xz760Y/GrFmzYv369YNeo7u7O7q7u/v/3NHREaeddlrs378/SktL89kuADBGOjs7o7KyMg4ePBhlZWXDcp+T8pnc09MTO3bsiBUrVvSPFRYWRm1tbbS0tAy6pqWlJerr6weM1dXVxS9/+cujXqexsTFuvfXWI8YrKyvz2S4AcBz4xz/+MTbhcuDAgejt7Y3y8vIB4+Xl5bFnz55B17S1tQ06v62t7ajXWbFixYDYOXjwYLznPe+Jffv2DdsDZ2jeqGfPfo09Z3H8cBbHF+dx/HjjFZN3vOMdw3afeYXLaMnlcpHL5Y4YLysr8w/hcaK0tNRZHCecxfHDWRxfnMfxo7Bw+D7EnNc9TZkyJYqKiqK9vX3AeHt7e1RUVAy6pqKiIq/5AABHk1e4FBcXx+zZs6O5ubl/rK+vL5qbm6OmpmbQNTU1NQPmR0Q8+uijR50PAHA0eb9UVF9fH4sXL445c+bE3LlzY+3atdHV1RVLliyJiIhFixbFjBkzorGxMSIibrjhhrj44ovjzjvvjMsuuyw2btwYTzzxRNxzzz3HfM1cLhcNDQ2DvnzE6HIWxw9ncfxwFscX53H8GImzyPvj0BER69ati+9973vR1tYWs2bNiu9///tRXV0dEREf//jHo6qqKu6///7++Q888EDcfPPN8eKLL8b73//+uP322+PSSy8dtgcBAEwMQwoXAICx4HcVAQDJEC4AQDKECwCQDOECACTjuAmXpqamqKqqipKSkqiuro7t27e/6fwHHnggzjrrrCgpKYlzzjkntmzZMko7Hf/yOYsNGzbERRddFJMnT47JkydHbW3tW54dxy7fvxdv2LhxYxQUFMT8+fNHdoMTSL5ncfDgwVi6dGlMmzYtcrlcnHnmmf49NUzyPYu1a9fGBz7wgTjxxBOjsrIyli1bFq+99too7Xb8+sMf/hDz5s2L6dOnR0FBwZv+DsI3bNu2LT7ykY9ELpeL973vfQM+gXzMsuPAxo0bs+Li4uy+++7L/vznP2dXX311duqpp2bt7e2Dzv/jH/+YFRUVZbfffnv29NNPZzfffHN2wgknZE8++eQo73z8yfcsrrjiiqypqSnbtWtXtnv37uyLX/xiVlZWlv31r38d5Z2PP/mexRteeOGFbMaMGdlFF12UfeYznxmdzY5z+Z5Fd3d3NmfOnOzSSy/NHnvsseyFF17Itm3blrW2to7yzseffM/ipz/9aZbL5bKf/vSn2QsvvJA98sgj2bRp07Jly5aN8s7Hny1btmQrV67MHnrooSwisocffvhN5+/duzc76aSTsvr6+uzpp5/OfvCDH2RFRUXZ1q1b87rucREuc+fOzZYuXdr/597e3mz69OlZY2PjoPM/97nPZZdddtmAserq6uxLX/rSiO5zIsj3LP7X4cOHs1NOOSX7yU9+MlJbnDCGchaHDx/OLrjgguxHP/pRtnjxYuEyTPI9ix/+8IfZ6aefnvX09IzWFieMfM9i6dKl2Sc+8YkBY/X19dmFF144ovucaI4lXL7xjW9kH/7whweMLViwIKurq8vrWmP+UlFPT0/s2LEjamtr+8cKCwujtrY2WlpaBl3T0tIyYH5ERF1d3VHnc2yGchb/69VXX43XX399WH8T6EQ01LP41re+FVOnTo0rr7xyNLY5IQzlLH71q19FTU1NLF26NMrLy+Pss8+O1atXR29v72hte1wayllccMEFsWPHjv6Xk/bu3RtbtmzxJahjYLh+do/5b4c+cOBA9Pb2Rnl5+YDx8vLy2LNnz6Br2traBp3f1tY2YvucCIZyFv/rxhtvjOnTpx/xDyf5GcpZPPbYY3HvvfdGa2vrKOxw4hjKWezduzd+97vfxRe+8IXYsmVLPPfcc/HlL385Xn/99WhoaBiNbY9LQzmLK664Ig4cOBAf+9jHIsuyOHz4cFx77bVx0003jcaW+X+O9rO7s7Mz/v3vf8eJJ554TPcz5s+4MH6sWbMmNm7cGA8//HCUlJSM9XYmlEOHDsXChQtjw4YNMWXKlLHezoTX19cXU6dOjXvuuSdmz54dCxYsiJUrV8b69evHemsTzrZt22L16tVx9913x86dO+Ohhx6KzZs3x2233TbWW2OIxvwZlylTpkRRUVG0t7cPGG9vb4+KiopB11RUVOQ1n2MzlLN4wx133BFr1qyJ3/72t3HuueeO5DYnhHzP4vnnn48XX3wx5s2b1z/W19cXERGTJk2KZ555Js4444yR3fQ4NZS/F9OmTYsTTjghioqK+sc++MEPRltbW/T09ERxcfGI7nm8GspZ3HLLLbFw4cK46qqrIiLinHPOia6urrjmmmti5cqVUVjov99Hy9F+dpeWlh7zsy0Rx8EzLsXFxTF79uxobm7uH+vr64vm5uaoqakZdE1NTc2A+RERjz766FHnc2yGchYREbfffnvcdtttsXXr1pgzZ85obHXcy/cszjrrrHjyySejtbW1//bpT386LrnkkmhtbY3KysrR3P64MpS/FxdeeGE899xz/fEYEfHss8/GtGnTRMvbMJSzePXVV4+IkzeCMvOr+kbVsP3szu99wyNj48aNWS6Xy+6///7s6aefzq655prs1FNPzdra2rIsy7KFCxdmy5cv75//xz/+MZs0aVJ2xx13ZLt3784aGhp8HHqY5HsWa9asyYqLi7MHH3ww+/vf/95/O3To0Fg9hHEj37P4Xz5VNHzyPYt9+/Zlp5xySvaVr3wle+aZZ7Jf//rX2dSpU7Nvf/vbY/UQxo18z6KhoSE75ZRTsp///OfZ3r17s9/85jfZGWeckX3uc58bq4cwbhw6dCjbtWtXtmvXriwisrvuuivbtWtX9pe//CXLsixbvnx5tnDhwv75b3wc+utf/3q2e/furKmpKd2PQ2dZlv3gBz/ITjvttKy4uDibO3du9qc//an/f7v44ouzxYsXD5j/i1/8IjvzzDOz4uLi7MMf/nC2efPmUd7x+JXPWbznPe/JIuKIW0NDw+hvfBzK9+/F/ydchle+Z/H4449n1dXVWS6Xy04//fTsO9/5Tnb48OFR3vX4lM9ZvP7669k3v/nN7IwzzshKSkqyysrK7Mtf/nL2z3/+c/Q3Ps78/ve/H/Tf/2/8/7948eLs4osvPmLNrFmzsuLi4uz000/PfvzjH+d93YIs81wZAJCGMX+PCwDAsRIuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQjP8DPZCkbwFa2SAAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find our model's learned parameters\n",
        "print(\"The model learned the following values for weights and bias:\")\n",
        "print(model.state_dict())\n",
        "print(\"\\nAnd the original values for weights and bias are:\")\n",
        "print(f\"weights: {weight}, bias: {bias}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn1yj3gYQ4pY",
        "outputId": "6faabe7d-8d1e-4296-c02d-8608a5dcf95e"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model learned the following values for weights and bias:\n",
            "OrderedDict([('weights', tensor([0.3067])), ('bias', tensor([0.9011]))])\n",
            "\n",
            "And the original values for weights and bias are:\n",
            "weights: 0.3, bias: 0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Make predictions with the trained model on the test data.\n",
        "  * Visualize these predictions against the original training and testing data (**note:** you may need to make sure the predictions are *not* on the GPU if you want to use non-CUDA-enabled libraries such as matplotlib to plot)."
      ],
      "metadata": {
        "id": "x4j4TM18jwa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with the model\n",
        "# 1. Set the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# 2. Setup the inference mode context manager\n",
        "with torch.inference_mode():\n",
        "  # 3. Make sure the calculations are done with the model and data on the same device\n",
        "  # in our case, we haven't setup device-agnostic code yet so our data and model are\n",
        "  # on the CPU by default.\n",
        "  # model_0.to(device)\n",
        "  # X_test = X_test.to(device)\n",
        "  y_preds = model(X_test)\n",
        "y_preds"
      ],
      "metadata": {
        "id": "bbMPK5Qjjyx_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92ad3b0e-6377-4f06-9aee-4080e760b56f"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.1464, 1.1495, 1.1525, 1.1556, 1.1587, 1.1617, 1.1648, 1.1679, 1.1709,\n",
              "        1.1740, 1.1771, 1.1801, 1.1832, 1.1863, 1.1893, 1.1924, 1.1955, 1.1985,\n",
              "        1.2016, 1.2047])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "source": [
        "# Plot the predictions (these may need to be on a specific device)\n",
        "plot_predictions(X_train, X_test, y_train, y_test, predictions=y_preds) # Pass X_train, X_test, y_train, and y_test"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "pIw_KzeMRLNL",
        "outputId": "3ad57b8a-3a8c-4377-c87e-a85f233dfbde"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0MAAAJGCAYAAACZel7oAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAANIdJREFUeJzt3X9wXXWZP/AnDTapIwmylJRiAAsCympaQGJVBrJTNwtM0zI7C6IDtQu6KIq04yJVpCzu1zoK3bKliqJuRUcBFZqMMGXZSmRZqgylmdEBWWpbqNgUcCS3rdJCcr5/ZBOITdJ7b3J/ntdr5k7I4dzL586cqX17Ps/71CRJkgQAAEDKTCn1AgAAAEpBGAIAAFJJGAIAAFJJGAIAAFJJGAIAAFJJGAIAAFJJGAIAAFLpkFIvYLIMDAzE73//+zj00EOjpqam1MsBAABKJEmS2L17d8ycOTOmTBn7/k/VhKHf//730dzcXOplAAAAZWLHjh3xlre8Zcx/XzVh6NBDD42IwS/c0NBQ4tUAAAClkslkorm5eTgjjKVqwtDQ1riGhgZhCAAAOOj4jAIFAAAglYQhAAAglYQhAAAglYQhAAAglYQhAAAglYQhAAAglYQhAAAglYQhAAAglYQhAAAglYQhAAAglYQhAAAglYQhAAAglYQhAAAglYQhAAAglYQhAAAglYQhAAAglYQhAAAglYQhAAAglYQhAAAglYQhAAAglYQhAABgYrq6IpYsGfxZQYQhAAAgf11dEQsWRKxePfizggKRMAQAAOTvwQcjamsj+vsHf3Z3l3pFWROGAACA/LW1vRaE+vsjzj671CvK2iGlXgAAAFDBOjoiOjsH7widffbg7xVCGAIAAA6uq2twS1xb24GBp6OjokLQkJy3yT300EMxf/78mDlzZtTU1MS6devGPf/uu++OD3zgAzF9+vRoaGiIuXPnxv3333/AeWvWrInjjjsu6uvro7W1NR599NFclwYAABRCBZckjCfnMLR3795oaWmJNWvWZHX+Qw89FB/4wAfivvvui02bNkVbW1vMnz8/Nm/ePHzOnXfeGUuXLo3ly5fH448/Hi0tLdHe3h7PP/98rssDAAAmWwWXJIynJkmSJO8319TEPffcEwsXLszpfaecckpceOGFcd1110VERGtra7z73e+OW265JSIiBgYGorm5OT71qU/FNddcM+pn7Nu3L/bt2zf8eyaTiebm5ujr64uGhob8vhAAAHCgoTtDQ4Gos7Ost8VlMplobGw8aDYoepvcwMBA7N69Ow4//PCIiNi/f39s2rQp5s2b99qipkyJefPmxcaNG8f8nBUrVkRjY+Pwq7m5ueBrBwCAVBoqSbjyyrIPQrkoehi68cYbY8+ePXHBBRdERMSLL74Y/f390dTUNOK8pqam6O3tHfNzli1bFn19fcOvHTt2FHTdAABQ9bq6IpYsGX0mqKMjYuXKqglCEUVuk/vBD34Q//Iv/xKdnZ1x5JFHTuiz6urqoq6ubpJWBgAAKff6rXCrVlXVHaCxFO3O0B133BGXXXZZ3HXXXSO2xB1xxBFRW1sbu3btGnH+rl27YsaMGcVaHgAApNsEShK6nuqKJeuXRNdTldUyV5Qw9MMf/jAWL14cP/zhD+O8884b8e+mTp0ap512WmzYsGH42MDAQGzYsCHmzp1bjOUBAABtba8Fof7+wQeoZqHrqa5YcMeCWP3o6lhwx4KKCkQ5b5Pbs2dPbNmyZfj3bdu2RU9PTxx++OFxzDHHxLJly+K5556L22+/PSIGt8YtWrQobr755mhtbR2eA5o2bVo0NjZGRMTSpUtj0aJFcfrpp8cZZ5wRq1atir1798bixYsn4zsCAAAHM1SS0N09GISy3CL34LYHo7amNvqT/qitqY3u7d3RcVJlbK/L+c7QY489FnPmzIk5c+ZExGCQmTNnznBN9s6dO+PZZ58dPv+b3/xmvPrqq3HFFVfEUUcdNfz69Kc/PXzOhRdeGDfeeGNcd911MXv27Ojp6Yn169cfUKoAAABM0CSXJLS9tW04CPUn/XH2cWdP3loLbELPGSon2XaJAwBAahXoeUFdT3VF9/buOPu4s8virlC22aCobXIAAEAJjVaSkGUY6nqqKx7c9mC0vbXtgMDTcVJHWYSgXBX9OUMAAECJpLAkYTzCEAAApMVQScKVV+a0RW60koRqIAwBAEC1UZKQFQUKAABQTVJSkjAeBQoAAJBGShKyZpscAABUEyUJWROGAACgEo01F6QkIWvCEAAAVJqhuaDVqwd/jhaIlCQclJkhAACoNBOYCxpLx0kd0fnBzoopSZgMwhAAAFSatraIVatynguKSF9JwnhUawMAQCXq6hq8I3T22Tm1xS24Y8HwVrjOD3ZWZfjJNhuYGQIAgHI1yQ9PTWNJwniEIQAAKEcHK0nIQxpLEsZjZggAAMpRAR6emsaShPGYGQIAgHI0dGdoKBBl+cygtMwFjcfMEAAAVDIPTy04YQgAAEppkksSzAVlzzY5AAAolTy3wh30Y5/qSvVcULbZQIECAACUSgFKEiLS9/DUfNkmBwAApdLW9loQ6u8ffIBqFoZKElY/ujoW3LEgup6aeO12GglDAABQKkoSSkoYAgCAQhqvICFCSUIJKVAAAIBCKVBBQoSShPEoUAAAgFKbQEFChJKEQrNNDgAACiXPgoQIJQnFIAwBAECh5FmQEKEkoRiEIQAAmKjxShLyKEiIUJJQDAoUAABgIpQklB0FCgAAUAxKEiqWbXIAADARShIqljAEAAAToSShYglDAACQDSUJVUeBAgAAHIyShIqiQAEAACbLBEoSxitIiFCSUEq2yQEAwMHkWZKgIKG8CUMAAHAweZYkKEgob8IQAAAMmeSSBAUJ5U2BAgAARBSsJEFBQvEpUAAAgFwUqCRBQUL5sk0OAAAilCSkkDAEAAARShJSSBgCACBdlCTwfxQoAACQHhMoSRhvLkhJQnlRoAAAAH8pz5KEobmg2praWPXLVdH5wc4RoUdJQmWyTQ4AgPTIsyTBXFB1EoYAAEiPPEsSzAVVJzNDAABUn66uwS1xbW2T8uDUCHNBlSTbbCAMAQBQXQpUkkDlyDYb2CYHAEB1Ga0kIQsenpo+whAAANVFSQJZEoYAAKguShLIkpkhAAAqk5IExqBAAQCA6qUkgXEoUAAAoHopSWASCEMAAFQeJQlMAmEIAIDKoySBSWBmCACA8qUkgTwoUAAAoLIpSSBPChQAAKhsShIoMGEIAIDypCSBAhOGAAAoT0oSKDAzQwAAlJaSBCaZAgUAAMqfkgQKQIECAADlT0kCJSQMAQBQOkoSKCFhCACA0lGSQAmZGQIAoLAKUJAQoSSBsSlQAACg9CZQkBChJIH8FKxA4aGHHor58+fHzJkzo6amJtatWzfu+Tt37owPfehDceKJJ8aUKVPiqquuOuCctWvXRk1NzYhXfX19rksDAKDc5FmQEKEkgcLLOQzt3bs3WlpaYs2aNVmdv2/fvpg+fXpce+210dLSMuZ5DQ0NsXPnzuHXM888k+vSAAAoN3kWJEQoSaDwDsn1Deecc06cc845WZ9/3HHHxc033xwREd/5znfGPK+mpiZmzJiR63IAAChnQwUJ3d2DQSiHLXJtb22LVb9cpSSBgsk5DBXKnj174thjj42BgYE49dRT40tf+lKccsopY56/b9++2Ldv3/DvmUymGMsEAGA045UkdHSMG4LGmgvqOKkjOj/YqSSBgimLau2TTjopvvOd70RnZ2d8//vfj4GBgXjve98bv/vd78Z8z4oVK6KxsXH41dzcXMQVAwAwbKgkYfXqwZ9d2c/2HGwuqOOkjljZvlIQoiDKIgzNnTs3Lrnkkpg9e3acddZZcffdd8f06dPjG9/4xpjvWbZsWfT19Q2/duzYUcQVAwAwbAIlCeaCKKWyCEN/6Q1veEPMmTMntmzZMuY5dXV10dDQMOIFAEAJTKAkwcNTKaWymRl6vf7+/vjVr34V5557bqmXAgDAkLHmgiZQkmAuiFLKOQzt2bNnxB2bbdu2RU9PTxx++OFxzDHHxLJly+K5556L22+/fficnp6e4fe+8MIL0dPTE1OnTo13vOMdERFxww03xHve85444YQT4qWXXoqvfvWr8cwzz8Rll102wa8HAMCkeP3DU1etOvDhqeOUJBzswakdJ3UIQZREzmHosccei7a2tuHfly5dGhERixYtirVr18bOnTvj2WefHfGeOXPmDP/zpk2b4gc/+EEce+yxsX379oiI+OMf/xgf/ehHo7e3N9785jfHaaedFo888shwWAIAoMRGmwvK4g7QUEFCbU1trPrlquj8YKfgQ9moSZIkKfUiJkMmk4nGxsbo6+szPwQAMNlef2eov//AO0NjWLJ+Sax+dPXwXNCVrVfGyvaVRVgwaZZtNijLAgUAAMrM0FzQlVdmHYQiFCRQ3twZAgCgoLqe6lKQQFFlmw2EIQAAJuxgJQlQTLbJAQBQFEMlCasfXR0L7lgQXU91lXpJkBVhCACACXlw24PDM0G1NbXRvb271EuCrAhDAABMiJIEKlXOzxkCAIDX6zipIzo/2KkkgYqjQAEAgKwoSaBSKFAAAGDSKEmgGglDAAAclJIEqpEwBADAQSlJoBopUAAA4KCUJFCNFCgAADCsqyviwQcj2toiOuQdKpQCBQAActLVFbFgQcTq1YM/u3QkUOWEIQAAImLwjlBtbUR//+DP7u5SrwgKSxgCACAiBrfGDQWh/v6Is88u9YqgsBQoAACkzFhzQR0dEZ2dg3eEzj7bzBDVT4ECAECKDM0FDd396ewUeqg+ChQAADiAuSB4jTAEAJAi5oLgNWaGAABSxFwQvEYYAgCoQuM9PLWjQwiCCNvkAACqjoenQnaEIQCAKqMkAbIjDAEAVBklCZAdM0MAAFVGSQJkRxgCAKhQShJgYmyTAwCoQEoSYOKEIQCACqQkASZOGAIAqEBKEmDizAwBAFQgJQkwccIQAEAZU5IAhWObHABAmVKSAIUlDAEAlCklCVBYwhAAQJlSkgCFZWYIAKBMKUmAwhKGAABKaLyChAglCVBItskBAJSIggQoLWEIAKBEFCRAaQlDAAAloiABSsvMEABAiShIgNIShgAACmy8kgQFCVA6tskBABSQkgQoX8IQAEABKUmA8iUMAQAUkJIEKF9mhgAAJsFYc0FKEqB81SRJkpR6EZMhk8lEY2Nj9PX1RUNDQ6mXAwCkyNBc0NDdn85OoQdKKdtsYJscAMAEmQuCyiQMAQBMkLkgqExmhgAAJshcEFQmYQgAIEsengrVxTY5AIAseHgqVB9hCAAgC0oSoPoIQwAAWVCSANXHzBAAQBaUJED1EYYAAF5HSQKkh21yAAD/R0kCpIswBADwf5QkQLoIQwAA/0dJAqSLmSEAgP+jJAHSRRgCAFJHSQIQYZscAJAyShKAIcIQAJAqShKAIcIQAJAqShKAIWaGAIBUUZIADBGGAICqpCQBOBjb5ACAqqMkAciGMAQAVB0lCUA2hCEAoOooSQCykXMYeuihh2L+/Pkxc+bMqKmpiXXr1o17/s6dO+NDH/pQnHjiiTFlypS46qqrRj3vRz/6UZx88slRX18f73znO+O+++7LdWkAABHxWknClVcO/jQfBIwm5zC0d+/eaGlpiTVr1mR1/r59+2L69Olx7bXXRktLy6jnPPLII3HRRRfFpZdeGps3b46FCxfGwoUL49e//nWuywMAUqSrK2LJktFngjo6IlauFISAsdUkSZLk/eaamrjnnnti4cKFWZ1/9tlnx+zZs2PVqlUjjl944YWxd+/e+OlPfzp87D3veU/Mnj07br311qw+O5PJRGNjY/T19UVDQ0O2XwEAqFBDJQlDW+HcAQKGZJsNymJmaOPGjTFv3rwRx9rb22Pjxo1jvmffvn2RyWRGvACA9FCSAExUWYSh3t7eaGpqGnGsqakpent7x3zPihUrorGxcfjV3Nxc6GUCAGVESQIwUWURhvKxbNmy6OvrG37t2LGj1EsCAApgrLkgJQnARB1S6gVERMyYMSN27do14tiuXbtixowZY76nrq4u6urqCr00AKCEXj8XtGrVgaGno0MIAvJXFneG5s6dGxs2bBhx7IEHHoi5c+eWaEUAQDkwFwQUUs53hvbs2RNbtmwZ/n3btm3R09MThx9+eBxzzDGxbNmyeO655+L2228fPqenp2f4vS+88EL09PTE1KlT4x3veEdERHz605+Os846K2666aY477zz4o477ojHHnssvvnNb07w6wEAlaytbfCOkLkgoBByrtbu7u6Otra2A44vWrQo1q5dGx/5yEdi+/bt0f26/+umpqbmgPOPPfbY2L59+/DvP/rRj+Laa6+N7du3x9ve9rb4yle+Eueee27W61KtDQDVqatr8I7Q2WfbEgdkJ9tsMKHnDJUTYQgAKlNX1+B2uLY2YQeYHBX1nCEAIJ2GChJWrx78+ZeNcQCFJAwBACWjIAEoJWEIACgZD04FSqksnjMEAKTT0INTFSQApSAMAQAFN15JggenAqVimxwAUFBKEoByJQwBAAWlJAEoV8IQAFBQShKAcmVmCAAoKCUJQLkShgCASaEkAag0tskBABOmJAGoRMIQADBhShKASiQMAQATpiQBqERmhgCACVOSAFQiYQgAyJqSBKCa2CYHAGRFSQJQbYQhACArShKAaiMMAQBZUZIAVBszQwDACGPNBSlJAKpNTZIkSakXMRkymUw0NjZGX19fNDQ0lHo5AFCRhuaChu7+dHYKPUDlyTYb2CYHAAwzFwSkiTAEAAwzFwSkiZkhAGCYuSAgTYQhAEghD08FsE0OAFLHw1MBBglDAJAyShIABglDAJAyShIABpkZAoCUUZIAMEgYAoAqpSQBYHy2yQFAFVKSAHBwwhAAVCElCQAHJwwBQBVSkgBwcGaGAKAKKUkAODhhCAAqmJIEgPzZJgcAFUpJAsDECEMAUKGUJABMjDAEABVKSQLAxJgZAoAKpSQBYGKEIQAoY+MVJEQoSQCYCNvkAKBMKUgAKCxhCADKlIIEgMIShgCgTClIACgsM0MAUKYUJAAUljAEACU2XkmCggSAwrFNDgBKSEkCQOkIQwBQQkoSAEpHGAKAElKSAFA6ZoYAoAjGmgtSkgBQOjVJkiSlXsRkyGQy0djYGH19fdHQ0FDq5QDAsKG5oKG7P52dQg9AIWWbDWyTA4ACMxcEUJ6EIQAoMHNBAOXJzBAAFJi5IIDyJAwBwCTx8FSAymKbHABMAg9PBag8whAATAIlCQCVRxgCgEmgJAGg8pgZAoBJoCQBoPIIQwCQAyUJANXDNjkAyJKSBIDqIgwBQJaUJABUF2EIALKkJAGgupgZAoAsKUkAqC7CEAD8BSUJAOlgmxwAvI6SBID0EIYA4HWUJACkhzAEAK+jJAEgPcwMAcDrKEkASA9hCIBUUpIAgG1yAKSOkgQAIoQhAFJISQIAEXmEoYceeijmz58fM2fOjJqamli3bt1B39Pd3R2nnnpq1NXVxQknnBBr164d8e+vv/76qKmpGfE6+eSTc10aAGRFSQIAEXmEob1790ZLS0usWbMmq/O3bdsW5513XrS1tUVPT09cddVVcdlll8X9998/4rxTTjkldu7cOfx6+OGHc10aAGRlqCThyisHf5oPAkinnAsUzjnnnDjnnHOyPv/WW2+Nt771rXHTTTdFRMTb3/72ePjhh+Pf/u3for29/bWFHHJIzJgxI9flAMCYlCQAMJ6Czwxt3Lgx5s2bN+JYe3t7bNy4ccSxp59+OmbOnBmzZs2KD3/4w/Hss8+O+7n79u2LTCYz4gUAQ5QkAHAwBQ9Dvb290dTUNOJYU1NTZDKZ+POf/xwREa2trbF27dpYv359fP3rX49t27bFmWeeGbt37x7zc1esWBGNjY3Dr+bm5oJ+DwAqi5IEAA6mLNrkzjnnnPiHf/iHeNe73hXt7e1x3333xUsvvRR33XXXmO9ZtmxZ9PX1Db927NhRxBUDUO6UJABwMAV/6OqMGTNi165dI47t2rUrGhoaYtq0aaO+57DDDosTTzwxtmzZMubn1tXVRV1d3aSuFYDKM9Zc0FBJQnf3YBAyHwTAXyr4naG5c+fGhg0bRhx74IEHYu7cuWO+Z8+ePfHb3/42jjrqqEIvD4AKdrC5oI6OiJUrBSEARpdzGNqzZ0/09PRET09PRAxWZ/f09AwXHixbtiwuueSS4fMvv/zy2Lp1a1x99dXxm9/8Jr72ta/FXXfdFUuWLBk+5zOf+Uz8/Oc/j+3bt8cjjzwS559/ftTW1sZFF100wa8HQDUzFwTAROQchh577LGYM2dOzJkzJyIili5dGnPmzInrrrsuIiJ27tw5ognurW99a9x7773xwAMPREtLS9x0003xrW99a0St9u9+97u46KKL4qSTTooLLrgg/uqv/ip+8YtfxPTp0yf6/QCoYuaCAJiImiRJklIvYjJkMplobGyMvr6+aGhoKPVyACiSri5zQQCMlG02KHiBAgBMxHgPTo3w8FQA8lcW1doAMBoPTgWgkIQhAMqWggQACkkYAqBsKUgAoJDMDAFQtjw4FYBCEoYAKLnxShIUJABQKLbJAVBSShIAKBVhCICSUpIAQKkIQwCUlJIEAErFzBAAJaUkAYBSEYYAKAolCQCUG9vkACg4JQkAlCNhCICCU5IAQDkShgAoOCUJAJQjM0MAFJySBADKkTAEwKRRkgBAJbFNDoBJoSQBgEojDAEwKZQkAFBphCEAJoWSBAAqjZkhAHIy1lyQkgQAKk1NkiRJqRcxGTKZTDQ2NkZfX180NDSUejkAVWloLmjo7k9np9ADQPnJNhvYJgdA1swFAVBNhCEAsmYuCIBqYmYIgKyZCwKgmghDABzAw1MBSAPb5AAYwcNTAUgLYQiAEZQkAJAWwhAAIyhJACAtzAwBMIKSBADSQhgCSCklCQCknW1yACmkJAEAhCGAVFKSAADCEEAqKUkAADNDAKmkJAEAhCGAqqYkAQDGZpscQJVSkgAA4xOGAKqUkgQAGJ8wBFCllCQAwPjMDAFUKSUJADA+YQiggo1XkBChJAEAxmObHECFUpAAABMjDAFUKAUJADAxwhBAhVKQAAATY2YIoEIpSACAiRGGAMrceCUJChIAIH+2yQGUMSUJAFA4whBAGVOSAACFIwwBlDElCQBQOGaGAMrAWHNBShIAoHBqkiRJSr2IyZDJZKKxsTH6+vqioaGh1MsByNrQXNDQ3Z/OTqEHACYi22xgmxxAiZkLAoDSEIYASsxcEACUhpkhgBIzFwQApSEMARSJh6cCQHmxTQ6gCDw8FQDKjzAEUARKEgCg/AhDAEWgJAEAyo+ZIYAiUJIAAOVHGAKYREoSAKBy2CYHMEmUJABAZRGGACaJkgQAqCzCEMAkUZIAAJXFzBDAJFGSAACVRRgCyJGSBACoDrbJAeRASQIAVA9hCCAHShIAoHoIQwA5UJIAANXDzBBADpQkAED1EIYARqEkAQCqX87b5B566KGYP39+zJw5M2pqamLdunUHfU93d3eceuqpUVdXFyeccEKsXbv2gHPWrFkTxx13XNTX10dra2s8+uijuS4NYFIoSQCAdMg5DO3duzdaWlpizZo1WZ2/bdu2OO+886KtrS16enriqquuissuuyzuv//+4XPuvPPOWLp0aSxfvjwef/zxaGlpifb29nj++edzXR7AhClJAIB0qEmSJMn7zTU1cc8998TChQvHPOezn/1s3HvvvfHrX/96+NgHP/jBeOmll2L9+vUREdHa2hrvfve745ZbbomIiIGBgWhubo5PfepTcc0112S1lkwmE42NjdHX1xcNDQ35fiWA4TtDQ4Gos9O2OACoJNlmg4K3yW3cuDHmzZs34lh7e3ts3LgxIiL2798fmzZtGnHOlClTYt68ecPnjGbfvn2RyWRGvAAmw1BJwpVXCkIAUM0KHoZ6e3ujqalpxLGmpqbIZDLx5z//OV588cXo7+8f9Zze3t4xP3fFihXR2Ng4/Gpubi7I+oHq1dUVsWTJ6DNBHR0RK1cKQgBQzSr2OUPLli2Lvr6+4deOHTtKvSSggihJAAAKXq09Y8aM2LVr14hju3btioaGhpg2bVrU1tZGbW3tqOfMmDFjzM+tq6uLurq6gqwZqH6jlSS4CwQA6VLwO0Nz586NDRs2jDj2wAMPxNy5cyMiYurUqXHaaaeNOGdgYCA2bNgwfA7AZGtrey0I9fcPPkAVAEiXnO8M7dmzJ7Zs2TL8+7Zt26KnpycOP/zwOOaYY2LZsmXx3HPPxe233x4REZdffnnccsstcfXVV8c//uM/xs9+9rO466674t577x3+jKVLl8aiRYvi9NNPjzPOOCNWrVoVe/fujcWLF0/CVwTSbKyHpw6VJHR3DwYhd4UAIH1yrtbu7u6Otra2A44vWrQo1q5dGx/5yEdi+/bt0f26B3N0d3fHkiVL4oknnoi3vOUt8YUvfCE+8pGPjHj/LbfcEl/96lejt7c3Zs+eHf/+7/8era2tWa9LtTbwl1RkA0A6ZZsNJvScoXIiDAF/acmSwYKEoe1wV1452BAHAFS3snnOEECpmAsCAMZT8DY5gFIxFwQAjEcYAiraWAUJQzo6hCAAYHS2yQEVy4NTAYCJEIaAijXag1MBALIlDAEVS0ECADARZoaAiqUgAQCYCGEIKHvjlSQoSAAA8mWbHFDWlCQAAIUiDAFlTUkCAFAowhBQ1pQkAACFYmYIKGtKEgCAQhGGgLKgJAEAKDbb5ICSU5IAAJSCMASUnJIEAKAUhCGg5JQkAAClYGYIKDklCQBAKQhDQNEoSQAAyoltckBRKEkAAMqNMAQUhZIEAKDcCENAUShJAADKjZkhoCiUJAAA5UYYAiaVkgQAoFLYJgdMGiUJAEAlEYaASaMkAQCoJMIQMGmUJAAAlcTMEJCzseaClCQAAJWkJkmSpNSLmAyZTCYaGxujr68vGhoaSr0cqFpDc0FDd386O4UeAKC8ZJsNbJMDcmIuCACoFsIQkBNzQQBAtTAzBOTEXBAAUC2EIWBUHp4KAFQ72+SAA3h4KgCQBsIQcAAlCQBAGghDwAGUJAAAaWBmCDiAkgQAIA2EIUgxJQkAQJrZJgcppSQBAEg7YQhSSkkCAJB2whCklJIEACDtzAxBSilJAADSThiCKqckAQBgdLbJQRVTkgAAMDZhCKqYkgQAgLEJQ1DFlCQAAIzNzBBUMSUJAABjE4agwo1XkBChJAEAYCy2yUEFU5AAAJA/YQgqmIIEAID8CUNQwRQkAADkz8wQVICx5oIUJAAA5K8mSZKk1IuYDJlMJhobG6Ovry8aGhpKvRyYNENzQUN3fzo7hR4AgPFkmw1sk4MyZy4IAKAwhCEoc+aCAAAKw8wQlDlzQQAAhSEMQZkY7+GpHpwKADD5bJODMuDhqQAAxScMQRlQkgAAUHzCEJQBJQkAAMVnZgjKgJIEAIDiE4agiJQkAACUD9vkoEiUJAAAlBdhCIpESQIAQHkRhqBIlCQAAJQXM0NQJEoSAADKizAEk0xJAgBAZbBNDiaRkgQAgMohDMEkUpIAAFA5hCGYREoSAAAqh5khmERKEgAAKkded4bWrFkTxx13XNTX10dra2s8+uijY577yiuvxA033BDHH3981NfXR0tLS6xfv37EOddff33U1NSMeJ188sn5LA2KoqsrYsmS0WeCOjoiVq4UhAAAyl3OYejOO++MpUuXxvLly+Pxxx+PlpaWaG9vj+eff37U86+99tr4xje+EatXr44nnngiLr/88jj//PNj8+bNI8475ZRTYufOncOvhx9+OL9vBAWmJAEAoDrkHIZWrlwZH/3oR2Px4sXxjne8I2699dZ44xvfGN/5zndGPf973/tefO5zn4tzzz03Zs2aFR//+Mfj3HPPjZtuumnEeYccckjMmDFj+HXEEUeMu459+/ZFJpMZ8YJiUJIAAFAdcgpD+/fvj02bNsW8efNe+4ApU2LevHmxcePGUd+zb9++qK+vH3Fs2rRpB9z5efrpp2PmzJkxa9as+PCHPxzPPvvsuGtZsWJFNDY2Dr+am5tz+SqQNyUJAADVIacw9OKLL0Z/f380NTWNON7U1BS9vb2jvqe9vT1WrlwZTz/9dAwMDMQDDzwQd999d+zcuXP4nNbW1li7dm2sX78+vv71r8e2bdvizDPPjN27d4+5lmXLlkVfX9/wa8eOHbl8FcjbUEnClVcO/jQbBABQmQreJnfzzTfHRz/60Tj55JOjpqYmjj/++Fi8ePGIbXXnnHPO8D+/613vitbW1jj22GPjrrvuiksvvXTUz62rq4u6urpCL58U6+oa3BLX1nZg4OnoEIIAACpdTneGjjjiiKitrY1du3aNOL5r166YMWPGqO+ZPn16rFu3Lvbu3RvPPPNM/OY3v4k3velNMWvWrDH/O4cddliceOKJsWXLllyWB5NGSQIAQPXLKQxNnTo1TjvttNiwYcPwsYGBgdiwYUPMnTt33PfW19fH0UcfHa+++mr85Cc/iQULFox57p49e+K3v/1tHHXUUbksDyaNkgQAgOqXc5vc0qVL47bbbovvfve78eSTT8bHP/7x2Lt3byxevDgiIi655JJYtmzZ8Pm//OUv4+67746tW7fGf//3f8ff/d3fxcDAQFx99dXD53zmM5+Jn//857F9+/Z45JFH4vzzz4/a2tq46KKLJuErQu6UJAAAVL+cZ4YuvPDCeOGFF+K6666L3t7emD17dqxfv364VOHZZ5+NKVNey1gvv/xyXHvttbF169Z405veFOeee25873vfi8MOO2z4nN/97ndx0UUXxR/+8IeYPn16vP/9749f/OIXMX369Il/QxjHWHNBQyUJ3d2DQch8EABA9alJkiQp9SImQyaTicbGxujr64uGhoZSL4cKMDQXNHT3RzMcAEB1yDYb5LxNDqqFuSAAgHQThkgtc0EAAOlW8OcMQbkyFwQAkG7CEFVtvAenRnh4KgBAmtkmR9Xy4FQAAMYjDFG1FCQAADAeYYiqpSABAIDxmBmiailIAABgPMIQFW+8kgQFCQAAjMU2OSqakgQAAPIlDFHRlCQAAJAvYYiKpiQBAIB8mRmioilJAAAgX8IQFUFJAgAAk802OcqekgQAAApBGKLsKUkAAKAQhCHKnpIEAAAKwcwQZU9JAgAAhSAMUTaUJAAAUEy2yVEWlCQAAFBswhBlQUkCAADFJgxRFpQkAABQbGaGKAtKEgAAKDZhiKJSkgAAQLmwTY6iUZIAAEA5EYYoGiUJAACUE2GIolGSAABAOTEzxKQbay5ISQIAAOWkJkmSpNSLmAyZTCYaGxujr68vGhoaSr2c1BqaCxq6+9PZKfQAAFBc2WYD2+SYVOaCAACoFMIQk8pcEAAAlcLMEJPKXBAAAJVCGCIvHp4KAECls02OnHl4KgAA1UAYImdKEgAAqAbCEDlTkgAAQDUwM0TOlCQAAFANhCHGpCQBAIBqZpsco1KSAABAtROGGJWSBAAAqp0wxKiUJAAAUO3MDDEqJQkAAFQ7YSjllCQAAJBWtsmlmJIEAADSTBhKMSUJAACkmTCUYkoSAABIMzNDKaYkAQCANBOGqtx4BQkRShIAAEgv2+SqmIIEAAAYmzBUxRQkAADA2IShKqYgAQAAxmZmqAqMNRekIAEAAMZWkyRJUupFTIZMJhONjY3R19cXDQ0NpV5O0QzNBQ3d/ensFHoAAEi3bLOBbXIVzlwQAADkRxiqcOaCAAAgP2aGKpy5IAAAyI8wVCHGe3iqB6cCAEDubJOrAB6eCgAAk08YqgBKEgAAYPIJQxVASQIAAEw+M0MVQEkCAABMPmGojChJAACA4rFNrkwoSQAAgOIShsqEkgQAACguYahMKEkAAIDiMjNUJpQkAABAcQlDRaYkAQAAyoNtckWkJAEAAMqHMFREShIAAKB85BWG1qxZE8cdd1zU19dHa2trPProo2Oe+8orr8QNN9wQxx9/fNTX10dLS0usX79+Qp9ZqZQkAABA+cg5DN15552xdOnSWL58eTz++OPR0tIS7e3t8fzzz496/rXXXhvf+MY3YvXq1fHEE0/E5ZdfHueff35s3rw578+sVEMlCVdeOfjTfBAAAJROTZIkSS5vaG1tjXe/+91xyy23RETEwMBANDc3x6c+9am45pprDjh/5syZ8fnPfz6uuOKK4WN///d/H9OmTYvvf//7eX3maDKZTDQ2NkZfX180NDTk8pUm3XglCQAAQGFlmw1yujO0f//+2LRpU8ybN++1D5gyJebNmxcbN24c9T379u2L+vr6EcemTZsWDz/8cN6fOfS5mUxmxKscKEkAAIDKkFMYevHFF6O/vz+amppGHG9qaore3t5R39Pe3h4rV66Mp59+OgYGBuKBBx6Iu+++O3bu3Jn3Z0ZErFixIhobG4dfzc3NuXyVglGSAAAAlaHgbXI333xzvO1tb4uTTz45pk6dGp/85Cdj8eLFMWXKxP7Ty5Yti76+vuHXjh07JmnFE6MkAQAAKkNOD1094ogjora2Nnbt2jXi+K5du2LGjBmjvmf69Omxbt26ePnll+MPf/hDzJw5M6655pqYNWtW3p8ZEVFXVxd1dXW5LL8ohkoSursHg5CZIQAAKE853Z6ZOnVqnHbaabFhw4bhYwMDA7Fhw4aYO3fuuO+tr6+Po48+Ol599dX4yU9+EgsWLJjwZ5arjo6IlSsFIQAAKGc53RmKiFi6dGksWrQoTj/99DjjjDNi1apVsXfv3li8eHFERFxyySVx9NFHx4oVKyIi4pe//GU899xzMXv27Hjuuefi+uuvj4GBgbj66quz/kwAAIDJlnMYuvDCC+OFF16I6667Lnp7e2P27Nmxfv364QKEZ599dsQ80MsvvxzXXnttbN26Nd70pjfFueeeG9/73vfisMMOy/ozAQAAJlvOzxkqV+X0nCEAAKB0CvKcIQAAgGohDAEAAKkkDAEAAKkkDAEAAKkkDAEAAKkkDAEAAKkkDAEAAKkkDAEAAKkkDAEAAKkkDAEAAKkkDAEAAKkkDAEAAKkkDAEAAKkkDAEAAKkkDAEAAKkkDAEAAKl0SKkXMFmSJImIiEwmU+KVAAAApTSUCYYywliqJgzt3r07IiKam5tLvBIAAKAc7N69OxobG8f89zXJweJShRgYGIjf//73ceihh0ZNTU1J15LJZKK5uTl27NgRDQ0NJV0LlcN1Qz5cN+TLtUM+XDfkoxTXTZIksXv37pg5c2ZMmTL2ZFDV3BmaMmVKvOUtbyn1MkZoaGjwBwU5c92QD9cN+XLtkA/XDfko9nUz3h2hIQoUAACAVBKGAACAVBKGCqCuri6WL18edXV1pV4KFcR1Qz5cN+TLtUM+XDfko5yvm6opUAAAAMiFO0MAAEAqCUMAAEAqCUMAAEAqCUMAAEAqCUMAAEAqCUN5WrNmTRx33HFRX18fra2t8eijj457/o9+9KM4+eSTo76+Pt75znfGfffdV6SVUk5yuW5uu+22OPPMM+PNb35zvPnNb4558+Yd9DqjOuX6582QO+64I2pqamLhwoWFXSBlK9dr56WXXoorrrgijjrqqKirq4sTTzzR/16lUK7XzapVq+Kkk06KadOmRXNzcyxZsiRefvnlIq2WcvDQQw/F/PnzY+bMmVFTUxPr1q076Hu6u7vj1FNPjbq6ujjhhBNi7dq1BV/naIShPNx5552xdOnSWL58eTz++OPR0tIS7e3t8fzzz496/iOPPBIXXXRRXHrppbF58+ZYuHBhLFy4MH79618XeeWUUq7XTXd3d1x00UXx4IMPxsaNG6O5uTn+9m//Np577rkir5xSyvW6GbJ9+/b4zGc+E2eeeWaRVkq5yfXa2b9/f3zgAx+I7du3x49//ON46qmn4rbbboujjz66yCunlHK9bn7wgx/ENddcE8uXL48nn3wyvv3tb8edd94Zn/vc54q8ckpp79690dLSEmvWrMnq/G3btsV5550XbW1t0dPTE1dddVVcdtllcf/99xd4paNIyNkZZ5yRXHHFFcO/9/f3JzNnzkxWrFgx6vkXXHBBct5554041tramvzTP/1TQddJecn1uvlLr776anLooYcm3/3udwu1RMpQPtfNq6++mrz3ve9NvvWtbyWLFi1KFixYUISVUm5yvXa+/vWvJ7NmzUr2799frCVShnK9bq644orkb/7mb0YcW7p0afK+972voOukfEVEcs8994x7ztVXX52ccsopI45deOGFSXt7ewFXNjp3hnK0f//+2LRpU8ybN2/42JQpU2LevHmxcePGUd+zcePGEedHRLS3t495PtUnn+vmL/3pT3+KV155JQ4//PBCLZMyk+91c8MNN8SRRx4Zl156aTGWSRnK59rp6uqKuXPnxhVXXBFNTU3x13/91/GlL30p+vv7i7VsSiyf6+a9731vbNq0aXgr3datW+O+++6Lc889tyhrpjKV09+NDyn6f7HCvfjii9Hf3x9NTU0jjjc1NcVvfvObUd/T29s76vm9vb0FWyflJZ/r5i999rOfjZkzZx7whwfVK5/r5uGHH45vf/vb0dPTU4QVUq7yuXa2bt0aP/vZz+LDH/5w3HfffbFly5b4xCc+Ea+88kosX768GMumxPK5bj70oQ/Fiy++GO9///sjSZJ49dVX4/LLL7dNjnGN9XfjTCYTf/7zn2PatGlFW4s7Q1ABvvzlL8cdd9wR99xzT9TX15d6OZSp3bt3x8UXXxy33XZbHHHEEaVeDhVmYGAgjjzyyPjmN78Zp512Wlx44YXx+c9/Pm699dZSL40y1t3dHV/60pfia1/7Wjz++ONx9913x7333htf/OIXS700yIo7Qzk64ogjora2Nnbt2jXi+K5du2LGjBmjvmfGjBk5nU/1yee6GXLjjTfGl7/85fiv//qveNe73lXIZVJmcr1ufvvb38b27dtj/vz5w8cGBgYiIuKQQw6Jp556Ko4//vjCLpqykM+fOUcddVS84Q1viNra2uFjb3/726O3tzf2798fU6dOLeiaKb18rpsvfOELcfHFF8dll10WERHvfOc7Y+/evfGxj30sPv/5z8eUKf5/dw401t+NGxoainpXKMKdoZxNnTo1TjvttNiwYcPwsYGBgdiwYUPMnTt31PfMnTt3xPkREQ888MCY51N98rluIiK+8pWvxBe/+MVYv359nH766cVYKmUk1+vm5JNPjl/96lfR09Mz/Oro6Bhu62lubi7m8imhfP7Med/73hdbtmwZDtAREf/7v/8bRx11lCCUEvlcN3/6058OCDxDgTpJksItlopWVn83LnplQxW44447krq6umTt2rXJE088kXzsYx9LDjvssKS3tzdJkiS5+OKLk2uuuWb4/P/5n/9JDjnkkOTGG29MnnzyyWT58uXJG97whuRXv/pVqb4CJZDrdfPlL385mTp1avLjH/842blz5/Br9+7dpfoKlECu181f0iaXXrleO88++2xy6KGHJp/85CeTp556KvnpT3+aHHnkkcm//uu/luorUAK5XjfLly9PDj300OSHP/xhsnXr1uQ///M/k+OPPz654IILSvUVKIHdu3cnmzdvTjZv3pxERLJy5cpk8+bNyTPPPJMkSZJcc801ycUXXzx8/tatW5M3vvGNyT//8z8nTz75ZLJmzZqktrY2Wb9+fdHXLgzlafXq1ckxxxyTTJ06NTnjjDOSX/ziF8P/7qyzzkoWLVo04vy77rorOfHEE5OpU6cmp5xySnLvvfcWecWUg1yum2OPPTaJiANey5cvL/7CKalc/7x5PWEo3XK9dh555JGktbU1qaurS2bNmpX8v//3/5JXX321yKum1HK5bl555ZXk+uuvT44//vikvr4+aW5uTj7xiU8kf/zjH4u/cErmwQcfHPXvLEPXyqJFi5KzzjrrgPfMnj07mTp1ajJr1qzkP/7jP4q+7iRJkpokcQ8TAABIHzNDAABAKglDAABAKglDAABAKglDAABAKglDAABAKglDAABAKglDAABAKglDAABAKglDAABAKglDAABAKglDAABAKv1/w7YA/814PLEAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Save your trained model's `state_dict()` to file.\n",
        "  * Create a new instance of your model class you made in 2. and load in the `state_dict()` you just saved to it.\n",
        "  * Perform predictions on your test data with the loaded model and confirm they match the original model predictions from 4."
      ],
      "metadata": {
        "id": "s2OnlMWKjzX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# 1. Create models directory\n",
        "\n",
        "\n",
        "# 2. Create model save path\n",
        "\n",
        "# 3. Save the model state dict\n"
      ],
      "metadata": {
        "id": "hgxhgD14qr-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new instance of model and load saved state dict (make sure to put it on the target device)\n"
      ],
      "metadata": {
        "id": "P9vTgiLRrJ7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with loaded model and compare them to the previous\n"
      ],
      "metadata": {
        "id": "8UGX3VebrVtI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}